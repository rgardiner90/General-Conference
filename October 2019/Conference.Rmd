---
title: "Conference"
author: "Richard G. Gardiner"
date: "4/7/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(tidytext)
library(rvest)
library(wordcloud)
library(readxl)
library(widyr)
library(igraph)
library(ggraph)
library(stringr)
library(gridExtra)
library(koRpus)

theme_set(theme_light())
```

## Getting Data
Getting list of all links
```{r}
conference_page <- read_html("https://www.churchofjesuschrist.org/general-conference?lang=eng&country=ca")

landing_page <- conference_page %>%
  html_nodes(".lumen-tile__link ") %>%
  html_attr("href") %>%
  tibble() %>%
  rename("link" = 1) %>%
  mutate(full_link = paste0('https://www.churchofjesuschrist.org', link))
```


Creating Function
```{r}
collect_speaker <- function(x) {
  
  url <- x
  
  individual <- read_html(url)
  
  title <- individual %>%
    html_nodes("#title1") %>%
    html_text()
  
  speaker <- individual %>%
    html_nodes("#author1") %>%
    html_text()

  text <- individual %>%
    html_nodes(".body-block p") %>%
    html_text()
  
  talk <- tibble(speaker = speaker, 
                    title = title, 
                    text = text) 
  
  return(talk)
}

collect_speaker(landing_page$full_link[2])
```

Creating dataset for analysis
```{r}
landing_page2 <- landing_page$full_link

talks_list <- lapply(landing_page2, collect_speaker)

talks <- talks_list %>%
  tibble() %>%
  unnest() 


session <- talks %>%
  distinct(title) %>%
  mutate(row_id = row_number(),
         session = case_when(
           row_id <= 7 ~ "Saturday Morning",
           row_id <= 16 ~ "Saturday Afternoon",
           row_id <= 22 ~ "Women's Session",
           row_id <= 28 ~ "Sunday Morning",
           TRUE ~ "Sunday Afternoon"
         )) %>%
  select(-row_id)


talks <- talks %>%
  left_join(session) %>%
  filter(!(str_detect(title, "Sustaining"))) %>%
  mutate(speaker = str_replace(speaker, "By ", ""))
```


```{r}
# write_csv(talks, "shiny_talks.csv")
```



## Analysis

First up to is to determine the most common words in confernece:

```{r}
# unigram
conference <- talks %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = "word") %>%
  filter(str_detect(word, "[a-z]"))

conference %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  mutate(word = str_to_title(word),
         word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  theme_light() +
  labs(x = "Number of Occurances", y = "Word",
       title = "Which Words Were Mentioned Most?")
```

Let's do the more artistic, but less rigorous, word cloud.
```{r}
conference %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

## Bigram

Counts: nothing interesting
```{r}
# bigram
conference_bigram <- talks %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!(word1 %in% stop_words$word),
         !(word2 %in% stop_words$word),
         str_detect(word1, "[a-z]"),
         str_detect(word2, "[a-z]")) %>%
  unite(bigram, word1, word2, sep = " ")

conference_bigram %>%
  count(bigram, sort = TRUE) %>%
  top_n(10, bigram) %>%
  ungroup() %>%
  mutate(bigram = fct_reorder(bigram, n)) %>%
  ggplot(aes(bigram, n)) +
  geom_col(show.legend = FALSE) +
  coord_flip() 

conference_bigram %>%
  group_by(session) %>%
  count(bigram, sort = TRUE) %>%
  top_n(10, bigram) %>%
  ungroup() %>%
  mutate(bigram = fct_reorder(bigram, n)) %>%
  ggplot(aes(bigram, n, fill = session)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~session, scale = "free_y")
```

Correlation

```{r}
# correlations within talks
word_cors <- conference %>%
  group_by(word) %>%
  add_count() %>%
  filter(n >= 20) %>%
  pairwise_cor(word, title, sort = TRUE) %>%
  filter(correlation != 1)

word_cors

# by session, nothign to ointeresting
conference %>%
  add_count(word) %>%
  filter(n >= 20) %>%
  pairwise_cor(word, session, sort = TRUE) %>%
  filter(correlation != 1) 
```

```{r}
word_cors %>%
  filter(item1 %in% c("church", "love", "gospel", "temple")) %>%
  group_by(item1) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(item2 = reorder_within(item2, correlation, item1)) %>%
  ggplot(aes(item2, correlation,  fill = item1)) +
  geom_col(show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap( ~ item1, scales = "free_y") +
  coord_flip() +
  theme_light() +
  labs(x = "Correlation", y = "Word", 
       title = "Correlation between Words")

# ggsave("Correlation between words.jpeg")
```

```{r}
set.seed(2016)

word_cors %>%
  filter(correlation > .50) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

# ggsave("network graph.jpeg")
```


```{r}
# creating necessary functions
count_bigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}

visualize_bigrams <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}

talks %>%
  count_bigrams() %>%
  filter(n > 5,
  !str_detect(word1, "\\d"),
  !str_detect(word2, "\\d")) %>%
  visualize_bigrams()
```



```{r}
talk_tf_idf <- conference %>%
  count(title, word) %>%
  bind_tf_idf(word, title, n)

nelson_talks <- conference %>%
  filter(str_detect(speaker, "Nelson"))

nelson_talks <- unique(nelson_talks$title)


talk_tf_idf %>%
  filter(title %in% nelson_talks) %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(title) %>%
  top_n(15) %>%
  ungroup() %>%
  ggplot(aes(x = word, y = tf_idf, fill = title)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Word", y = "Most Important Words",
     caption = "Scores are tf-idf Statistics", 
     title = "Most Important Words for President Nelson's Talks") +
  facet_wrap(~ title, scales = "free") +
  coord_flip() +
  theme_classic()

# ggsave("Nelson's Talks.jpeg")

conference %>%
  count(title, word) %>%
  tidylo::bind_log_odds(title, word, n) %>%
  filter(title %in% nelson_talks) %>%
  arrange(desc(log_odds)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(title) %>%
  top_n(15) %>%
  ungroup() %>%
  ggplot(aes(x = word, y = log_odds, fill = title)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Word", y = "Most Important Words",
     caption = "Scores are Weighted Logged Odds Statistics", 
     title = "Most Important Words for President Nelson's Talks") +
  facet_wrap(~ title, scales = "free") +
  coord_flip() +
  theme_classic()
```

```{r}
author_tf_idf <- conference %>%
  count(speaker, word) %>%
  bind_tf_idf(word, speaker, n)

author_tf_idf %>%
  filter(str_detect(speaker,  c("Nelson", "Cordon", "Bednar"))) %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(speaker) %>%
  top_n(5, tf_idf) %>%
  ungroup() %>%
  mutate(word = reorder_within(word, tf_idf, speaker)) %>%
  ggplot(aes(x = word, y = tf_idf, fill = speaker)) +
  geom_col(show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~ speaker, scales = "free") +
  labs(x = "Word", y = "Most Important Words",
       caption = "Scores are tf-idf Statistics",
       title = "Most Important Words for Three Speakers") +
  coord_flip() +
  theme_classic()

# ggsave("tf idf of Speakers.jpeg")

conference %>%
  count(speaker, word) %>%
  tidylo::bind_log_odds(speaker, word, n) %>%
  filter(str_detect(speaker,  c("Nelson", "Cordon", "Bednar"))) %>%
  arrange(desc(log_odds)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(speaker) %>%
  top_n(5, log_odds) %>%
  ungroup() %>%
  mutate(word = reorder_within(word, log_odds, speaker)) %>%
  ggplot(aes(x = word, y = log_odds, fill = speaker)) +
  geom_col(show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~ speaker, scales = "free") +
  labs(x = "Word", y = "Most Important Words",
       caption = "Scores are Weighted Log-Odds",
       title = "Most Important Words for Three Speakers") +
  coord_flip() +
  theme_classic()
```





```{r}
conference_sentiments <- conference %>%
  inner_join(get_sentiments("afinn"), by = "word")
  
conference_sentiments %>%
  group_by(title, speaker) %>%
  summarize(total_sentiment = sum(value)) %>% 
  ungroup() %>%
  mutate(title = fct_reorder(title, total_sentiment)) %>%
  ggplot(aes(title, total_sentiment, label = speaker, fill = total_sentiment)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = -.1) +
  ylim(0,400) +
  labs(x = "", y = "Net Sentiment Score",
       title = "Which Talk was the 'Most Positive'?") +
  coord_flip() +
  theme_light()

# ggsave("Talks sentiment.jpeg")

conference_sentiments %>%
  group_by(title, speaker) %>%
  ungroup() %>%
  filter(str_detect(speaker, "Christofferson")) %>%
  count(word, value) %>%
  mutate(weighted_score = value * n) %>%
  mutate(word = fct_reorder(word, weighted_score)) %>%
  arrange(desc(weighted_score)) %>%
  head(12) %>%
  ggplot(aes(x = word, y = weighted_score, fill = weighted_score)) +
  geom_col(show.legend = FALSE) +
  labs(x = "", y = "Weighted Sentiment Score",
       title = "Which Words Made Christofferson's the Most Positive?") +
  coord_flip() +
  theme_light()

# ggsave("ballard talk sentiment.jpeg")

conference_sentiments %>%
  group_by(title, speaker) %>%
  ungroup() %>%
  filter(str_detect(speaker, "Christofferson")) %>%
  count(word, value) %>%
  mutate(weighted_score = value * n) %>%
  mutate(word = fct_reorder(word, weighted_score)) %>%
  arrange(weighted_score) %>%
  head(12) %>%
  ggplot(aes(x = word, y = weighted_score, fill = weighted_score)) +
  geom_col(show.legend = FALSE) +
  labs(x = "", y = "Weighted Sentiment Score",
       title = "Which Words Made Christofferson the Least Positive?") +
  coord_flip() +
  scale_fill_gradient(low = "red", high = "#FFE4E1") +
  theme_light()

# ggsave("Christofferson talks.jpeg")
```



## Topic Modeling

Now I want to do topic modeling for the conference.  First I have to change our tidy text format into a document term matrix.  This requires doing a quick count of each word in a talk, then specifying the talk, word, and count.  Then we can use the `LDA` function to make 5 topics.  Then I tidy it back up and add the beta value which gives a probability that this specific word fits into that topic.  Thus, in our 5 topic category, every word will have 5 rows.
```{r}
library(topicmodels)


conference_dtm <- conference %>%
  count(title, word) %>%
  cast_dtm(title, word, n)


conference_lda <- LDA(conference_dtm, k = 5, control = list(seed = 2019))

conference_topic <- tidy(conference_lda, matrix = "beta")
conference_topic
```

### Visualizing the five topics

This is by far the hardest part of the analysis so far.

First thing I wanted to do was look at the most relevant words, so I kepts the top 100 words for each topic (decided by each beta).  Then I filled in all NAs with a 0.

```{r}
betas <- conference_topic %>%
  group_by(topic) %>%
  top_n(100, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  spread(topic, beta)

betas[is.na(betas)] <- 0  
```


Now here comes the triick part. First thing i did was to gather back the data frame so that there were only three columns: term, topic, and beta with multiple rows for a word.  Then I grouped everything by term, then I arranged everything by term and then by beta, so that we would have words grouped together with the highest beta on top and the lowest beta on bottom of the grouping.  That part is the key.  Because then I use the `first` function from dplyr to figure out which topic-term combo had the highest beta.  By saving it into another variable I was able to filter the entire dataset to only include one row per word, and that word was associated with a single topic. Thus, I basically made the system decide where a word should go (forcing a prediction).  After that I simply took a handful of the top values for each topic and fit in a nifty trick that helps me sort bar charts that are ordered over facets (big shout out to [this post](https://drsimonj.svbtle.com/ordering-categories-within-ggplot2-facets)).  It is a work around, but it appears to work.  

Lastly, I graph the results.
```{r}
topic_term_choice <- betas %>%
  gather(topic, beta, -term) %>%
  group_by(term) %>%
  arrange(term, desc(beta)) %>%
  mutate(first = first(beta)) %>%
  filter(beta == first) %>%
  ungroup() %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, beta) %>%
  mutate(order = row_number())

ggplot(topic_term_choice, aes(order, beta, fill = topic)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_x_continuous(
    breaks = topic_term_choice$order,
    labels = topic_term_choice$term,
    expand = c(0,0)) +
  coord_flip() 

# ggsave("topics.jpeg")
```

```{r}
conference_docs <- tidy(conference_lda, matrix = "gamma")
```


## Readability

```{r}
authors <- unique(talks$speaker)

authors <- function(author) {
  author <- talks %>%
    filter(speaker == author) %>%
    select(text) 
}

talks %>%
  filter(str_detect(speaker, "Eyring")) %>%
  select(text) %>%
  write.table("eyring.txt")

talks %>%
  filter(str_detect(speaker, "Nelson")) %>%
  select(text) %>%
  write.table("nelson.txt")

talks %>%
  filter(str_detect(speaker, "Dallin H. Oaks")) %>%
  select(text) %>%
  write.table("oaks.txt")
```



All of this came from the this [website tutorial](https://reaktanz.de/R/pckg/koRpus/koRpus_vignette.html)
```{r}
library(koRpus.lang.en)
```

First we want to tokenize a text document.  I actually created a text document of Bednar's talk.  
```{r}
eyring <- tokenize("eyring.txt",
                   lang = "en",
                   doc_id = "Eyring")
oaks <- tokenize("oaks.txt",
                 lang = "en",
                 doc_id = "Oaks")
nelson <- tokenize("nelson.txt",
                   lang = "en",
                   doc_id = "Nelson")
```


Now I am told we want to do some hyphenation.  This method is apparently not perfect, but I am doing to keep moving on.  Otherwise, I have to fix the problems myself using `correct.hyph` and this is just a demo.
```{r}
hyph_eyring <- hyphen(eyring)
hyph_oaks <- hyphen(oaks)
hyph_nelson <- hyphen(nelson)

# head(hyphenText(hyph_eyring)) 
```

Now we should be able to feed it into the `readability()` function to calculate the real interest!

```{r}
readability_eyring <- readability(eyring, hyphen = hyph_eyring)
readability_oaks <- readability(oaks, hyphen = hyph_oaks)
readability_nelson <- readability(nelson, hyphen = hyph_nelson)
```

```{r}
summary(readability_eyring)

rd_eyring <- readability_eyring@ARI$grade
rd_oaks <- readability_oaks@ARI$grade
rd_nelson <- readability_nelson@ARI$grade

readability_scores <- rbind(rd_eyring, rd_oaks, rd_nelson)
names <- c("Eyring", "Oaks", "Nelson")

readability_scores <- cbind(readability_scores, names)
readability_scores <- as.data.frame(readability_scores, names)

readability_scores %>%
  rename("GradeLevel" = 1) %>%
  mutate(GradeLevel = as.numeric(GradeLevel)) %>%
  mutate(names = fct_reorder(names, GradeLevel)) %>%
  ggplot(aes(names, GradeLevel)) +
  geom_col() +
  labs(x = "", y ="Grade Level - Automated Readability Index",
       title = "How Difficult is it to understand the First Presidency Talks?")

# ggsave("readability.jpeg")
```

## NEXT STEPS

```{r}
## ENGLISH_SYLLABLE_COUNT()
## Version 1.02.  Tyler Kendall, June 10, 2014
## This function counts the number of syllables in 'ortho'
## Converted to R from SLAAP (http://ncslaap.lib.ncsu.edu/) version in PHP 
## Originally based on Greg Fast's Lingua::EN::Syllable Perl Module
## That had an error rate of ~15% (though almost all errors off by just one syllable)
## This achieves close to 100% but will err on unusual words which are not
## in the lists of exceptions.
## Reference/Citation
##   Kendall, Tyler (2013). Speech rate, pause, and language variation: Studies in corpus sociophonetics. Basingstoke, UK: Palgrave Macmillan. [ http://ncslaap.lib.ncsu.edu/speechrateandpause/ ]
## Usage examples (first source this file) 
##   english_syllable_count("Happy times are here again!") # == 7
##   english_syllable_count(c("Happy", "times", "are", "here", "again!")) # == 7
##   sapply(c("Happy", "times", "are", "here", "again!"), english_syllable_count) # == c(2, 1, 1, 1, 2) # with names
count_syllables <- function(ortho) {
    
    # Can add words to these lists of 2 syllable and 3 syllable 'exceptions'
    # Note that final -e is stripped before checking these lists!
    Specials.2 <- c('every', 'different', 'family', 'girl', 'girls', 'world', 'worlds', 'bein', 'being', 'something', 'mkay', 'mayb')
    Specials.3 <- c('anyon', 'everyon') # final -e is dropped	
    
    # Regular Expression exceptions
    # SubSyl - remove a syllable from the count for each sub-string match
    SubSyl <- c('cial',
                'tia',
                'cius',
                'cious',
                'giu',              # belgium!
                'ion',
                'iou',
                '^every',           # every, but also everything, everybody
                'sia$',
                '.ely$',            # absolutely! (but not ely!)
                '[^szaeiou]es$',    # fates, but not sasses
                '[^tdaeiou]ed$',    # trapped, but not fated
                '^ninet',           # nineteen, ninety
                '^awe'				# awesome
    )
    
    # AddSyl - add a syllable to the count for each sub-string match
    AddSyl <- c('ia',
                'rie[rt]',
                'dien',
                'ieth',
                'iu',
                'io',
                'ii',
                'ienc',	      # ambience, science, ...
                'les?$',
                '[aeiouym][bp]l$',  # -Vble, plus -mble and -Vple
                '[aeiou]{3}',       # agreeable
                'ndl(ed)?$',        # handle, handled
                'mpl(ed)?$',	    # trample, trampled
                '^mc',				# McEnery
                'ism$',             # -isms
                '([^aeiouy])\\1l(ed)?$',  # middle twiddle battle bottle, etc.
                '[^l]lien',         # alien, salient [1]
                '^coa[dglx].',      # [2]
                '[^gq]ua[^aeiou]',  # i think this fixes more than it breaks
                '[sd]nt$',          # couldn't, didn't, hasn't, wasn't,...
                '\\wshes$',          # add one back for esh (since it's -'d)
                '\\wches$',          #  and for affricate (witches)
                '\\wges$',           #  and voiced (ages)
                '\\wces$',	      #  and sibilant 'c's (places)
                '\\w[aeiouy]ing[s]?$'   # vowels before -ing = hiatus
    )
    
    tot_syls <- 0
    ortho.l <- tolower(ortho)
    stripchars <- "[:'\\[\\]]"
    ortho.cl <- gsub(stripchars, "", ortho.l, perl=T)
    spacechars <- "[\\W_]" # replace other non-word chars with space
    ortho.cl <- gsub(spacechars, " ", ortho.cl, perl=T)
    ortho.vec <- unlist(strsplit(ortho.cl, " ", perl=T))
    ortho.vec <- ortho.vec[ortho.vec!=""]
    for (w in ortho.vec) {
        w <- gsub("e$", "", w, perl=T) # strip final -e
        syl <- 0
        # is word in the 2 syllable exception list?
        if (w %in% Specials.2) {
            syl <- 2
            
            # is word in the 3 syllable exception list?
        } else if (w %in% Specials.3) {
            syl <- 3
            
            # if not, than check the different parts...
        } else {
            for (pat in SubSyl) {
                if (length(grep(pat, w, perl=T))>=1) 
                    syl <- syl - 1
            }
            for (pat in AddSyl) {
                if (length(grep(pat, w, perl=T))>=1) 
                    syl <- syl + 1
            }
            if (nchar(w)==1) {
                syl <- 1
            } else {
                chnk <- unlist(strsplit(w, "[^aeiouy:]+"))
                chnk <- chnk[chnk!=""]
                syl <- syl + length(chnk)
                if (syl==0) syl <- 1
            }
        }
        tot_syls <- tot_syls + syl
    }
    tot_syls
}
```



```{r}
talks_sentences <- talks %>%
  unnest_tokens(word, text, drop = FALSE) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  filter(str_detect(word, "[a-z]")) %>%
  rowwise() %>%
  mutate(n_syllables = count_syllables(word)) %>%
  ungroup() 

results <- left_join(talks_sentences %>%
                         group_by(title, speaker) %>%
                         summarise(n_sentences = n_distinct(sentence)),
                     talks_sentences %>% 
                         group_by(title, speaker) %>% 
                         filter(n_syllables >= 3) %>% 
                         summarise(n_polysyllables = n())) %>%
    mutate(SMOG = 1.0430 * sqrt(30 * n_polysyllables/n_sentences) + 3.1291)

results
```

```{r}
results %>%
  ungroup() %>%
  mutate(title = fct_reorder(title, SMOG)) %>%
  ggplot(aes(x = title, y = SMOG)) +
  geom_col() +
  coord_flip()
```

