---
title: "Conference"
author: "Richard G. Gardiner"
date: "4/7/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(tidytext)
library(wordcloud)
library(readxl)
library(widyr)
library(igraph)
library(ggraph)
library(stringr)
library(gridExtra)
```

Steps necessary:

```{r}
conference_raw <- read_excel("Conference Talks.xlsx") %>%
  mutate(year = 2019,
         conference = "April") 


conference_raw %>%
  count(Talk)

numbers <- seq(1:100)


# unigram
conference <- conference_raw %>%
  unnest_tokens(word, Text) %>%
  anti_join(stop_words, by = "word") %>%
  filter(!(word %in% c(numbers)))



conference %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  theme_light() +
  labs(x = "Number of Occurances", y = "Word",
       title = "Which Words Were Mentioned Most?")

conference %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

## Bigram

Counts: nothing interesting
```{r}
# bigram
conference_bigram <- conference_raw %>%
  unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!(word1 %in% stop_words$word),
         !(word2 %in% stop_words$word),
         !(word1 %in% numbers),
         !(word2 %in% numbers)) %>%
  unite(bigram, word1, word2, sep = " ")

conference_bigram %>%
  group_by(Session) %>%
  count(bigram, sort = TRUE) %>%
  top_n(10, bigram) %>%
  ungroup() %>%
  mutate(bigram = fct_reorder(bigram, n)) %>%
  ggplot(aes(bigram, n, fill = Session)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~Session, scale = "free_y")
```

Correlation

```{r}
word_cors <- conference %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, Talk, sort = TRUE) %>%
  filter(correlation != 1)

word_cors
```

```{r}
data$carb <- factor(data$carb, levels = data$carb[order(-data$mpg)])


word_cors %>%
  filter(item1 %in% c("church", "love", "gospel", "temple")) %>%
  group_by(item1) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, desc(correlation))) %>% 
  ggplot(aes(item2, correlation,  fill = item1)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ item1, scales = "free_y") +
  coord_flip() +
  theme_light() +
  labs(x = "Correlation", y = "Word", 
       title = "Correlation between Words")

# ggsave("Correlation between words.jpeg")
```

```{r}
set.seed(2016)

word_cors %>%
  filter(correlation > .50) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

# ggsave("network graph.jpeg")
```


```{r}
# creating necessary functions
count_bigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}

visualize_bigrams <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}

conference_raw %>%
  count_bigrams() %>%
  filter(n > 5,
  !str_detect(word1, "\\d"),
  !str_detect(word2, "\\d")) %>%
  visualize_bigrams()
```



```{r}
talk_tf_idf <- conference %>%
  count(Talk, word) %>%
  bind_tf_idf(word, Talk, n)

nelson_talks <- conference[conference$Author == "Russell M. Nelson",]
nelson_talks <- unique(nelson_talks$Talk)


talk_tf_idf %>%
  filter(Talk %in% nelson_talks) %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(Talk) %>%
  top_n(15) %>%
  ungroup() %>%
  ggplot(aes(x = word, y = tf_idf, fill = Talk)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Word", y = "Most Important Words",
     caption = "Scores are tf-idf Statistics", 
     title = "Most Important Words for President Nelson's Talks") +
  facet_wrap(~ Talk, scales = "free") +
  coord_flip() +
  theme_classic()

ggsave("Nelson's Talks.jpeg")
```

```{r}
author_tf_idf <- conference %>%
  count(Author, word) %>%
  bind_tf_idf(word, Author, n)

author_tf_idf %>%
  filter(Author %in% c("Russell M. Nelson", "Becky Craven", "Jeffrey R. Holland")) %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(Author) %>%
  top_n(15) %>%
  ungroup() %>%
  ggplot(aes(x = word, y = tf_idf, fill = Author)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ Author, scales = "free") +
  labs(x = "Word", y = "Most Important Words",
       caption = "Scores are tf-idf Statistics",
       title = "Most Important Words for Three Speakers") +
  coord_flip() +
  theme_classic()

ggsave("tf idf of Speakers.jpeg")
```


```{r}
conference_sentiments <- conference %>%
  inner_join(get_sentiments("afinn"), by = "word")
  
conference_sentiments %>%
  group_by(Talk, Author) %>%
  summarize(total_sentiment = sum(score)) %>% 
  ungroup() %>%
  mutate(Talk = fct_reorder(Talk, total_sentiment)) %>%
  ggplot(aes(Talk, total_sentiment, label = Author, fill = total_sentiment)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = -.1) +
  ylim(0,400) +
  labs(x = "", y = "Net Sentiment Score",
       title = "Which Talk was the 'Most Positive'?") +
  coord_flip() +
  theme_light()

ggsave("Talks sentiment.jpeg")

conference_sentiments %>%
  group_by(Talk, Author) %>%
  ungroup() %>%
  filter(Author == "M. Russell Ballard") %>%
  count(word, score) %>%
  mutate(weighted_score = score * n) %>%
  mutate(word = fct_reorder(word, weighted_score)) %>%
  arrange(desc(weighted_score)) %>%
  head(12) %>%
  ggplot(aes(x = word, y = weighted_score, fill = weighted_score)) +
  geom_col(show.legend = FALSE) +
  labs(x = "", y = "Weighted Sentiment Score",
       title = "Which Words Made Ballard the Most Positive?") +
  coord_flip() +
  theme_light()

ggsave("ballard talk sentiment.jpeg")

conference_sentiments %>%
  group_by(Talk, Author) %>%
  ungroup() %>%
  filter(Author == "D. Todd Christofferson") %>%
  count(word, score) %>%
  mutate(weighted_score = score * n) %>%
  mutate(word = fct_reorder(word, weighted_score)) %>%
  arrange(weighted_score) %>%
  head(12) %>%
  ggplot(aes(x = word, y = weighted_score, fill = weighted_score)) +
  geom_col(show.legend = FALSE) +
  labs(x = "", y = "Weighted Sentiment Score",
       title = "Which Words Made Ballard the Least Positive?") +
  coord_flip() +
  scale_fill_gradient(low = "red", high = "#FFE4E1") +
  theme_light()

ggsave("Christofferson talks.jpeg")
```



## Topic Modeling

Now I want to do topic modeling for the conference.  First I have to change our tidy text format into a document term matrix.  This requires doing a quick count of each word in a talk, then specifying the talk, word, and count.  Then we can use the `LDA` function to make 5 topics.  Then I tidy it back up and add the beta value which gives a probability that this specific word fits into that topic.  Thus, in our 5 topic category, every word will have 5 rows.
```{r}
library(topicmodels)


conference_dtm <- conference %>%
  count(Talk, word) %>%
  cast_dtm(Talk, word, n)


conference_lda <- LDA(conference_dtm, k = 5, control = list(seed = 2019))

conference_topic <- tidy(conference_lda, matrix = "beta")
conference_topic
```

### Visualizing the five topics

This is by far the hardest part of the analysis so far.

First thing I wanted to do was look at the most relevant words, so I kepts the top 100 words for each topic (decided by each beta).  Then I filled in all NAs with a 0.

```{r}
betas <- conference_topic %>%
  group_by(topic) %>%
  top_n(100, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  spread(topic, beta)

betas[is.na(betas)] <- 0  
```


Now here comes the triick part. First thing i did was to gather back the data frame so that there were only three columns: term, topic, and beta with multiple rows for a word.  Then I grouped everything by term, then I arranged everything by term and then by beta, so that we would have words grouped together with the highest beta on top and the lowest beta on bottom of the grouping.  That part is the key.  Because then I use the `first` function from dplyr to figure out which topic-term combo had the highest beta.  By saving it into another variable I was able to filter the entire dataset to only include one row per word, and that word was associated with a single topic. Thus, I basically made the system decide where a word should go (forcing a prediction).  After that I simply took a handful of the top values for each topic and fit in a nifty trick that helps me sort bar charts that are ordered over facets (big shout out to [this post](https://drsimonj.svbtle.com/ordering-categories-within-ggplot2-facets)).  It is a work around, but it appears to work.  

Lastly, I graph the results.
```{r}
topic_term_choice <- betas %>%
  gather(topic, beta, -term) %>%
  group_by(term) %>%
  arrange(term, desc(beta)) %>%
  mutate(first = first(beta)) %>%
  filter(beta == first) %>%
  ungroup() %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, beta) %>%
  mutate(order = row_number())

ggplot(topic_term_choice, aes(order, beta, fill = topic)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_x_continuous(
    breaks = topic_term_choice$order,
    labels = topic_term_choice$term,
    expand = c(0,0)) +
  coord_flip()

# ggsave("topics.jpeg")
```

```{r}
conference_docs <- tidy(conference_lda, matrix = "gamma")
```


6. topic modeling
    - Use this informed maybe by word clouds
    
    
    